import gymnasium as gym
from gymnasium import spaces
import pennylane as qml
from pennylane import numpy as np
from scipy.stats import entropy
import matplotlib.pyplot as plt
import matplotlib.animation as animation

class ProbabilityV0(gym.Env):
    """
    Quantum probability-matching environment for reinforcement learning.

    This environment challenges an agent to construct a parameterized quantum circuit 
    (via an ansatz) such that the resulting probability distribution over computational 
    basis states matches a given target distribution.

    The environment supports reinforcement learning and optimization workflows:
    - The **state/observation** is the probability distribution generated by the current circuit.
    - The **action** is a continuous update (perturbation) to the current ansatz parameters.
    - The **reward** is based on the closeness of the circuit distribution to the target 
      distribution, using a weighted combination of KL divergence and L2 error, along with 
      an optional step penalty to encourage faster convergence.

    Parameters
    ----------
    n_qubits : int
        Number of qubits in the quantum circuit.
    target_distribution : np.ndarray
        Desired probability distribution over computational basis states. 
        Must sum to 1 and have length `2**n_qubits`.
    ansatz : callable, optional
        Parameterized quantum circuit ansatz. It should take the form 
        `ansatz(params, wires)`. If `None`, a default single-qubit rotation ansatz is used.
    max_steps : int, default=100
        Maximum number of steps before the episode terminates.
    tolerance : float, default=-1e3
        Termination condition based on reaching a sufficiently high reward.
    alpha : float, default=0.5
        Weight balancing KL divergence vs. L2 error in the reward function.
    beta : float, default=0.01
        Penalty coefficient applied per time step (to encourage shorter trajectories).

    Spaces
    ------
    action_space : gymnasium.spaces.Box
        Continuous action space of shape `(n_params,)` with values in [-0.1, 0.1], 
        corresponding to small parameter updates.
    observation_space : gymnasium.spaces.Box
        Probability distribution over `2**n_qubits` basis states (values in [0,1], summing to 1).

    Reward
    ------
    reward = -(alpha * KL(target || probs) + (1 - alpha) * L2(target - probs)) - beta * step

    - KL divergence encourages matching the shape of the target distribution.
    - L2 error ensures sensitivity to per-basis-state mismatches.
    - Step penalty discourages unnecessarily long episodes.

    Methods
    -------
    cost_fn(params):
        Computes the cost (negative reward) for a given set of circuit parameters.
    step(action):
        Applies an action (parameter update), evaluates the new distribution, 
        computes reward, and returns (observation, reward, done, info).
    reset():
        Resets the environment with random parameters and returns the initial distribution.
    render(mode="human"):
        Visualizes the target distribution vs. the current circuit distribution.
    animate(save_path=None):
        Generates an animation showing the trajectory of the circuit distribution 
        toward the target distribution across steps, with reward values.

    Notes
    -----
    - This environment is designed for hybrid reinforcement learning and 
      quantum-classical optimization.
    - It can be used as a benchmark for QRL algorithms that need to learn 
      quantum probability distributions.
    - The ansatz can be customized to explore different circuit architectures.
    """

    metadata = {"render.modes": ["human"]}

    def __init__(self, 
                 n_qubits: int,
                 target_distribution: np.ndarray,
                 ansatz=None,**kwargs):
        super(ProbabilityV0, self).__init__()

        assert np.isclose(np.sum(target_distribution), 1.0), \
            "Target distribution must sum to 1."
        self.n_qubits = n_qubits
        self.target_distribution = target_distribution
        self.max_steps = kwargs.get("max_steps", 100)
        self.tolerance = kwargs.get("tolerance", -1e3)
        self.alpha = kwargs.get("alpha", 0.5)  # weight for KL vs L2
        self.beta = kwargs.get("beta", 0.01)    # step penalty weight

        # Define PennyLane device
        self.dev = qml.device("default.qubit", wires=self.n_qubits)

        # If no ansatz is provided, define a simple one
        if ansatz is None:
            def default_ansatz(params, wires):
                for i, w in enumerate(wires):
                    qml.RY(params[i], wires=w)
            self.ansatz = default_ansatz
            self.n_params = self.n_qubits
        else:
            self.ansatz = ansatz
            try:
                self.n_params = ansatz.n_params  # If ansatz object has attribute
            except:
                raise ValueError("Please specify ansatz with n_params attribute.")

        # QNode
        @qml.qnode(self.dev)
        def circuit(params):
            self.ansatz(params, wires=range(self.n_qubits))
            return qml.probs(wires=range(self.n_qubits))
        self.circuit = circuit

        # Spaces
        self.action_space = spaces.Box(low=-0.1, high=0.1, shape=(self.n_params,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(2**self.n_qubits,), dtype=np.float32)

        # Internal state
        self.params = np.random.uniform(0, 2*np.pi, size=self.n_params)
        self.current_step = 0
        self.history = []
        self.rewards = []

    def cost_fn(self, params):
        probs = self.circuit(params)

        # KL divergence (target || probs)
        kl_div = np.sum(self.target_distribution * np.log((self.target_distribution + 1e-10) / (probs + 1e-10)))

        # L2 error
        l2_error = np.linalg.norm(self.target_distribution - probs, ord=2)

        # Reward
        reward = -(self.alpha * kl_div + (1 - self.alpha) * l2_error)

        return -reward

    def step(self, action):
        self.params = (self.params + action)  # keep params bounded
        self.current_step += 1

        probs = self.circuit(self.params)
        reward = self.cost_fn(self.params)

        done = reward < self.tolerance or self.current_step >= self.max_steps
        self.history.append(probs)
        self.rewards.append(reward)

        return probs, reward, done, {}



    def reset(self):
        self.params = np.random.uniform(0, 2*np.pi, size=self.n_params)
        self.current_step = 0
        self.history = []
        self.rewards = []
        return self.params, {}

    def render(self, mode="human"):
        if mode == "human":
            plt.figure(figsize=(10, 5))
            x = np.arange(2**self.n_qubits)

            plt.bar(x - 0.2, self.target_distribution, width=0.4, label="Target")

            if len(self.history) > 0:
                plt.bar(x + 0.2, self.history[-1], width=0.4, label="Current")

            plt.xlabel("Basis states")
            plt.ylabel("Probability")
            plt.title(f"Step {self.current_step}")
            plt.legend()
            plt.show()

    def animate(self, save_path=None):
        """
        Create an animation showing how the distribution evolves over steps,
        including reward values in the title.
        """
        fig, ax = plt.subplots(figsize=(10, 5))
        x = np.arange(2**self.n_qubits)
        width = 0.4

        target_bar = ax.bar(x - 0.2, self.target_distribution, width=width, label="Target")
        current_bar = ax.bar(x + 0.2, self.history[0], width=width, label="Prediction")

        ax.set_ylim(0, 1)
        ax.set_xticks(range(len(self.history[0])))
        ax.set_xticklabels([f"|{i}âŸ©" for i in range(len(self.history[0]))])
        ax.set_xlabel("Basis states")
        ax.set_ylabel("Probability")
        ax.legend()
        def update(frame):
            probs = self.history[frame]
            for bar, new_height in zip(current_bar, probs):
                bar.set_height(new_height)
            ax.set_title(f"Step {frame} | Reward: {np.array(self.rewards[frame].item()):.4f}")
            return current_bar

        ani = animation.FuncAnimation(fig, update, frames=len(self.history), blit=False)

        if save_path:
            ani.save(save_path, writer="ffmpeg", fps=2)
        else:
            plt.show()




if __name__ == "__main__":
    n_qubits = 2
    target_distribution = np.array([0.25, 0.25, 0.25, 0.25])  # Example target distribution

    # initialize environment
    env = ProbabilityV0(
        n_qubits=n_qubits,
        target_distribution=target_distribution,
        alpha=0.7,   # KL vs L2 weight
        beta=0.01,   # step penalty
        max_steps=100
    )

    # Reset environment
    params, _ = env.reset()


    # Use PennyLane's optimizer
    opt = qml.GradientDescentOptimizer(stepsize=0.2)

    # params = env.params.copy()
    for step in range(env.max_steps):
        params, cost_val = opt.step_and_cost(env.cost_fn, params)
        probs = env.circuit(params)

        # Save history for rendering
        env.history.append(probs)
        env.params = params  # update env params
        reward = -cost_val
        env.rewards.append(-cost_val)
        print(f"Step {step+1}: Reward = {reward:.4f}")

        if reward > -1e-2:  # close to perfect
            break

    # Show final distribution
    # env.render()

    # Animate the full evolution
    env.animate(save_path="results/core/probability_v0.mp4")
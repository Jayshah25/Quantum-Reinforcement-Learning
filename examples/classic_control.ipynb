{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Control Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore classic-control enviornments from the **gymnasium** python library. This set includes the following five enviornments:\n",
    "\n",
    "1. Acrobot\n",
    "\n",
    "2. Cart Pole\n",
    "\n",
    "3. Mountain Car Continuous\n",
    "\n",
    "4. Mountain Car\n",
    "\n",
    "5. Pendulum\n",
    "\n",
    "We will solve each of these environments using a simple classical agent (random sampling) and then using `BattleEnv` Wrapper from the `qrl` library for a battle between the classical and quantum agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/acrobot.gif\" alt=\"Description\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Acrobot environment is based on Sutton’s work in “Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding” and Sutton and Barto’s book. The system consists of two links connected linearly to form a chain, with one end of the chain fixed. The joint between the two links is actuated. The goal is to apply torques on the actuated joint to swing the free end of the linear chain above a given height while starting from the initial state of hanging downwards.\n",
    "\n",
    "As seen in the Gif: two blue links connected by two green joints. The joint in between the two links is actuated. The goal is to swing the free end of the outer-link to reach the target height (black horizontal line above system) by applying torque on the actuator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Space**: The action is discrete, deterministic, and represents the torque applied on the actuated joint between the two links -> {-1, 0, 1}\n",
    "\n",
    "**Observation Space**: The observation is a ndarray with shape (6,) that provides information about the two rotational joint angles as well as their angular velocities: Cosine of $\\theta_1$ [-1,1], Sine of $\\theta_1$[-1,1], Cosine of $\\theta_2$ [-1,1], Sine of $\\theta_2$[-1,1], Angular Velocity of $\\theta_1$[~ -12.567 (-4 $\\pi$), ~ 12.567 (4 $\\pi$)], Angular velocity of $\\theta_2$[~ -28.274 (-9 $\\pi$), ~ 28.274 (9 $\\pi$)].\n",
    "\n",
    "Here, \n",
    "\n",
    "* $\\theta_1$ : angle of the first joint, where an angle of 0 indicates the first link is pointing directly downwards.\n",
    "\n",
    "* $\\theta_2$: relative to the angle of the first link. An angle of 0 corresponds to having the same angle between the two links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rewards**\n",
    "\n",
    "* Objective: The goal is to have the free end reach a designated target height in as few steps as possible\n",
    "\n",
    "* All steps that do not reach the goal incur a reward of -1. \n",
    "\n",
    "* Achieving the target height results in termination with a reward of 0. \n",
    "\n",
    "* The reward threshold is -100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can the episode end?**\n",
    "\n",
    "The episode ends if one of the following occurs:\n",
    "\n",
    "1. **Termination**: The free end reaches the target height, which is constructed as: -cos($\\theta_1$) - cos($\\theta_2$ + $\\theta_1$) > 1.0\n",
    "\n",
    "2. **Truncation**: Episode length is greater than 500 (200 for v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Available Versions**\n",
    "\n",
    "* **v1**: Maximum number of steps increased from 200 to 500. The observation space for v0 provided direct readings of theta1 and theta2 in radians, having a range of [-$\\pi$, $\\pi$]. The v1 observation space as described here provides the sine and cosine of each angle instead.\n",
    "\n",
    "* **v0**: Initial versions release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**WE SOLVE THE LATEST v1 VERSION IN THIS EXAMPLE**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Agent (Random Sampling) in Acrobot environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10 - Total Reward: -500.0\n",
      "Episode 2/10 - Total Reward: -500.0\n",
      "Episode 3/10 - Total Reward: -500.0\n",
      "Episode 4/10 - Total Reward: -500.0\n",
      "Episode 5/10 - Total Reward: -500.0\n",
      "Episode 6/10 - Total Reward: -500.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7/10 - Total Reward: -500.0\n",
      "Episode 8/10 - Total Reward: -500.0\n",
      "Episode 9/10 - Total Reward: -500.0\n",
      "Episode 10/10 - Total Reward: -500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:   0%|          | 2/499 [11:31<47:43:22, 345.68s/it, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video videos_best/Acrobot-v1/classical.mp4.\n",
      "MoviePy - Writing video videos_best/Acrobot-v1/classical.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:   0%|          | 2/499 [11:33<47:53:04, 346.85s/it, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready videos_best/Acrobot-v1/classical.mp4\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "from moviepy import ImageSequenceClip\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "env_id=\"Acrobot-v1\" \n",
    "agent_type=\"classical\", \n",
    "episodes=10\n",
    "MAX_STEPS=500\n",
    "env = gym.make(env_id,render_mode=\"rgb_array\")\n",
    "action_space = env.action_space\n",
    "classical_agent_frames = {} #{episode_num:list_of_frames}\n",
    "reward_history = {} #{episode_num:reward_value}\n",
    "\n",
    "best_reward = float(\"-inf\")\n",
    "best_actions = []\n",
    "\n",
    "# Run episodes and track best\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    actions = []\n",
    "    frames_list = []\n",
    "    for _ in range(MAX_STEPS):\n",
    "        action = action_space.sample()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "\n",
    "        actions.append(action)\n",
    "        frames_list.append(frame)\n",
    "\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {ep+1}/{episodes} - Total Reward: {total_reward}\")\n",
    "    classical_agent_frames[ep+1] = frames_list\n",
    "    reward_history[ep+1] = total_reward\n",
    "\n",
    "    if total_reward > best_reward:\n",
    "        best_reward = total_reward\n",
    "        best_actions = actions.copy()\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "# get franes for the best episode (highest reward)\n",
    "best_episode = max(reward_history, key=reward_history.get)\n",
    "best_frames = classical_agent_frames.get(best_episode)\n",
    "annotated_best_frames = []\n",
    "\n",
    "# post processing\n",
    "for frame in best_frames:\n",
    "    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)     # Convert to BGR (OpenCV format)\n",
    "\n",
    "    # Add episode number on the frames\n",
    "    cv2.putText(img=frame_bgr,\n",
    "                text=f\"Classical Agent - Episode {best_episode}\",\n",
    "                org=(10, 30),\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=1,\n",
    "                color=(0, 0, 0),\n",
    "                thickness=2,\n",
    "                lineType=cv2.LINE_AA)\n",
    "\n",
    "    # Add total reward value on the frames\n",
    "    cv2.putText(img=frame_bgr,\n",
    "            text=f\"Total Reward: {best_reward}\",\n",
    "            org=(10, 65),\n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=1,\n",
    "            color=(0, 0, 0),\n",
    "            thickness=2,\n",
    "            lineType=cv2.LINE_AA)\n",
    "    \n",
    "    # Convert back to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    annotated_best_frames.append(frame_rgb)\n",
    "\n",
    "video_path = r\"videos_best/Acrobot-v1/classical.mp4\"\n",
    "clip = ImageSequenceClip(annotated_best_frames, fps=30)\n",
    "clip.write_videofile(video_path, codec='libx264')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car COntinuous "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
